{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600002860503",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# MNIST playground"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Overview\n",
    "In this notebook we will use **Tensorflow 2** to build simple network and train it on the well known\n",
    "[MNIST dataset](http://yann.lecun.com/exdb/mnist/). Later, we will improve created model by tuning hyperparameters, first ''by hand'' and later using **Keras Tuner**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Import modules"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import random\n",
    "import time\n",
    "import keras\n",
    "import IPython"
   ]
  },
  {
   "source": [
    "## Download and prepare the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "dataset = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load already randomized data\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "print(\"Training set data shape:\", X_train.shape)\n",
    "print(\"Training set target shape:\", Y_train.shape)\n",
    "# Convert rank one array to column array\n",
    "Y_train = Y_train.reshape(-1, 1)\n",
    "print(\"Labels shape after transformation:\", Y_train.shape)\n",
    "print(\"Test set data shape:\", X_test.shape)\n",
    "Y_test = Y_test.reshape(-1, 1)\n",
    "print(\"Test set target shape:\", Y_test.shape)"
   ]
  },
  {
   "source": [
    "### Single example from dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(X_train[10].reshape(28, 28), interpolation='nearest')\n",
    "print(Y_train[10])"
   ]
  },
  {
   "source": [
    "### Flatten input data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0],-1)\n",
    "print(X_train.shape)\n",
    "X_test = X_test.reshape(X_test.shape[0],-1)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "source": [
    "### Normalize data\n",
    "In case of the **MNIST** dataset it is common to scale the input data by dividing them by 255.0.  We will instead use Z normalization â€” this method transfer easier to other **ML** problems. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "print(X_train.mean(), X_test.mean())\n",
    "# Find mean and standard deviation\n",
    "scaler.fit(X_train)\n",
    "# print(scaler.mean_)\n",
    "# Use same mean, sd for scaling training and test data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print(X_train.mean(), X_test.mean())"
   ]
  },
  {
   "source": [
    "### Change output to categorial\n",
    "We will use One-hot Encoding to represent target"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(Y_train[0])\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train)\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test)\n",
    "print(Y_train[0])"
   ]
  },
  {
   "source": [
    "## Ploting functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curve(epochs, hist, list_of_metrics):\n",
    "  \"\"\"Plot a curve of  metrics vs. epoch.\n",
    "  Arguments:\n",
    "  epochs -- epochs list\n",
    "  hist -- training history given as pd.DataFrame\n",
    "  list_of_metics -- list of metrics to plot\n",
    "\n",
    "  metrics names should be as given in: https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#define_the_model_and_metrics\n",
    "  \"\"\"  \n",
    "  plt.figure()\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Value\")\n",
    "\n",
    "  # Plot given metrics\n",
    "  for m in list_of_metrics:\n",
    "    x = hist[m]\n",
    "    plt.plot(epochs[1:], x[1:], label=m)\n",
    "\n",
    "  plt.legend()"
   ]
  },
  {
   "source": [
    "## First Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Layers dimensions\n",
    "input_dim = X_train.shape[1]\n",
    "hl1_dim = 64\n",
    "hl2_dim = 64\n",
    "hl3_dim = 64\n",
    "output_layer_dim = Y_train.shape[1]\n",
    "\n",
    "# Create model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Add hidden layers  \n",
    "model.add(tf.keras.layers.Dense(units=hl1_dim,\n",
    "                                     input_dim=input_dim,\n",
    "                                     activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(units=hl2_dim,\n",
    "                                      activation='elu'))\n",
    "model.add(tf.keras.layers.Dense(units=hl3_dim,\n",
    "                                      activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "# We will use softmax -- is standard for classification problem\n",
    "# Network output looks like probabilistic distribution\n",
    "model.add(tf.keras.layers.Dense(units=output_layer_dim, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "### Compile model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "# Use loss=\"categorical_crossentropy\" for One-hot Encoding\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=\"categorical_crossentropy\", metrics=['accuracy', tf.keras.metrics.MSE])"
   ]
  },
  {
   "source": [
    "### Trainig"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First batch gradient descent\n",
    "batch_size = X_train.shape[0]\n",
    "epochs_number = 50\n",
    "print(\"Batch size used to train gradient:\", batch_size)\n",
    "validation_split_size = 0.2\n",
    "history = model.fit(x=X_train, y=Y_train, batch_size=batch_size,\n",
    "                      epochs=epochs_number, shuffle=True, \n",
    "                      validation_split=validation_split_size)"
   ]
  },
  {
   "source": [
    "### Model accuracy\n",
    "Show learning history and plot learning curves"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = history.epoch\n",
    "hist = pd.DataFrame(history.history)\n",
    "print(hist.head())\n",
    "print(hist[['accuracy', 'val_accuracy']].iloc[[-1]])\n",
    "# Plot a graph of the metric vs. epochs\n",
    "list_of_metrics_to_plot = ['accuracy', 'val_accuracy']\n",
    "plot_curve(epochs, hist, list_of_metrics_to_plot)"
   ]
  },
  {
   "source": [
    "### Evaluete model on test set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", model.evaluate(X_test, Y_test)[1])"
   ]
  },
  {
   "source": [
    "## Building model with regularization\n",
    "In our case difference between training accuracy and dev set accuracy was relatively large. Adding regularization to the model might diminish the difference."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For now set constant regularyzation and droput rates\n",
    "regularization_rate = 0.01\n",
    "droput_rate = 0.2\n",
    "# Build simple network\n",
    "# Regularyzation rates are same in every layer\n",
    "# just as droput rates\n",
    "def build_model():\n",
    "    \"\"\"Function builds simple network with regularization\n",
    "    \n",
    "    using global variables:\n",
    "    hl1_dim, hl2_dim, hl3_dim, regularization_rate and droput_rate\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(units=hl1_dim,\n",
    "                                        input_dim=input_dim,\n",
    "                                        activation='relu',\n",
    "                                        kernel_regularizer=tf.keras.regularizers.l2(regularization_rate)),\n",
    "        tf.keras.layers.Dropout(droput_rate),\n",
    "        tf.keras.layers.Dense(units=hl2_dim,\n",
    "                                        activation='elu',\n",
    "                                        kernel_regularizer=tf.keras.regularizers.l1(regularization_rate)),\n",
    "        \n",
    "        tf.keras.layers.Dropout(droput_rate),\n",
    "        tf.keras.layers.Dense(units=hl3_dim,\n",
    "                                        activation='relu',\n",
    "                                        kernel_regularizer=tf.keras.regularizers.l2(regularization_rate)),\n",
    "        tf.keras.layers.Dense(units=output_layer_dim, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "def compile_model(model):\n",
    "    \"\"\"Compile model\"\"\"\n",
    "    model.compile(optimizer=optimizer,\n",
    "                loss=\"categorical_crossentropy\", metrics=['accuracy', tf.keras.metrics.MSE])\n",
    "\n",
    "\n",
    "def train_model(model, verbose=1):\n",
    "    \"\"\"Train model\n",
    "    \n",
    "    using global variables:\n",
    "    X_train, Y_train, batch_size, epochs_number, validation_split_size\n",
    "    \"\"\"\n",
    "    history = model.fit(x=X_train, y=Y_train, batch_size=batch_size,\n",
    "                        epochs=epochs_number, shuffle=True, \n",
    "                        validation_split=validation_split_size, verbose=verbose)\n",
    "    return history"
   ]
  },
  {
   "source": [
    "### Test model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs_number = 15\n",
    "# Use mini batch gradient descent\n",
    "# batch size should fit in GPU memory\n",
    "batch_size = 64\n",
    "# Build and train model with regularization\n",
    "model = build_model()\n",
    "compile_model(model)\n",
    "history = train_model(model)\n",
    "epochs = history.epoch\n",
    "hist = pd.DataFrame(history.history)\n",
    "print(hist[['accuracy', 'val_accuracy']].iloc[[-1]])\n",
    "# Plot a graph of the metric vs. epochs.\n",
    "plot_curve(epochs, hist, list_of_metrics_to_plot)"
   ]
  },
  {
   "source": [
    "## Simple hyperparameters tuning\n",
    "We will start by simple hyperparameters tuning. In each iteration choose simultaneously (this way we can test more models) new set of hyperparameter, use them to create and train model. After a given time function will return best working hyperparameters. Because of randomness included in training, values found by function might perform poorly. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_learning_rate():\n",
    "    \"\"\"Generate random learning rate from logaritmic distribution\"\"\"\n",
    "\n",
    "    # Firs choose r from (-4, 0)\n",
    "    r = random.random() * -4\n",
    "    # Return 10^{r}, that way will generate on average\n",
    "    # smaller learning rates than simple linear distribution\n",
    "    return 10**r\n",
    "\n",
    "def random_regularization_rate():\n",
    "    \"\"\"Generate random regularization rate from linear distirbution\"\"\"\n",
    "    return random.random()\n",
    "\n",
    "def random_droput_rat():\n",
    "    \"\"\"Generate random droput rate from linear distirbution\"\"\"\n",
    "    return random.random() * 0.75\n",
    "\n",
    "def tune_hyperparameters(avaible_time):\n",
    "    \"\"\"Function returns best found set of hyperparameters in given time\n",
    "    \n",
    "    Returns dictionary where:\n",
    "    'lr' -- best found learning rate\n",
    "    'rr' -- best found regularization_rate\n",
    "    'dr' -- best found droput_rate\n",
    "    'acc' -- best trained model accuracy\n",
    "    \"\"\"\n",
    "    # Initiate best hyperparameters\n",
    "    best_learning_rate = 0\n",
    "    best_regularization_rate = 0\n",
    "    best_droput_rate = 0\n",
    "    best_val_accuracy = 0\n",
    "    st_time = time.time()\n",
    "    while time.time() - st_time < avaible_time:\n",
    "        # Generate new set of hyperparameters\n",
    "        learning_rate = random_learning_rate()\n",
    "        regularization_rate = random_regularization_rate()\n",
    "        droput_rate = random_droput_rat()\n",
    "        # Create and train new model\n",
    "        model = build_model()\n",
    "        compile_model(model)\n",
    "        history = train_model(model, verbose=0)\n",
    "        hist = pd.DataFrame(history.history)\n",
    "        new_model_val_accuracy = hist['val_accuracy'].iloc[-1]\n",
    "        # If new model has higher accuracy than currently higest\n",
    "        # update hyperparameters\n",
    "        if new_model_val_accuracy > best_val_accuracy:\n",
    "            best_learning_rate = learning_rate\n",
    "            best_regularization_rate = regularization_rate\n",
    "            best_droput_rate = droput_rate\n",
    "            best_val_accuracy = new_model_val_accuracy\n",
    "    \n",
    "    return {\n",
    "        'lr':best_learning_rate,\n",
    "        'rr':best_regularization_rate,\n",
    "        'dr':best_droput_rate,\n",
    "        'acc':best_val_accuracy\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avaible_time = 3600\n",
    "parameters = tune_hyperparameters(avaible_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(parameters)"
   ]
  },
  {
   "source": [
    "### Build model using found hyperparametes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate, regularization_rate, droput_rate = parameters['lr'], parameters['rr'], parameters['dr']\n",
    "epochs_number = 50\n",
    "batch_size=64\n",
    "model = build_model()\n",
    "compile_model(model)\n",
    "history = train_model(model)\n",
    "epochs = history.epoch\n",
    "hist = pd.DataFrame(history.history)\n",
    "print(hist[['accuracy', 'val_accuracy']].iloc[[-1]])\n",
    "# Plot a curve of the metric vs. epochs.\n",
    "list_of_metrics_to_plot = ['accuracy', 'val_accuracy']\n",
    "plot_curve(epochs, hist, list_of_metrics_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", model.evaluate(X_test, Y_test)[1])"
   ]
  },
  {
   "source": [
    "## Hyperparameters tuning with Keras Tuner"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kerastuner as kt"
   ]
  },
  {
   "source": [
    "### Define model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_hp(hp):\n",
    "  \"\"\"KT model builder function\"\"\"\n",
    "  model = keras.Sequential()\n",
    "  \n",
    "  # Tuned variables\n",
    "  hp_learning_rate = hp.Float('learning_rate', 1e-4, 1e-1, sampling='log')\n",
    "  hp_regularization_rate = hp.Float('regularization_rate', 0, 1, sampling='linear')\n",
    "  hp_droput_rate = hp.Float('droput_rate', 0, 0.75, sampling='linear')\n",
    "  hp_units1 = hp.Int('units1', min_value = 32, max_value = 512, step = 32)\n",
    "  hp_units2 = hp.Int('units2', min_value = 32, max_value = 512, step = 32)\n",
    "  hp_units3 = hp.Int('units3', min_value = 32, max_value = 512, step = 32)\n",
    "  hp_first_activation = hp.Choice('activation1', ['relu', 'tanh'])\n",
    "\n",
    "  # Build model using tuned variables  \n",
    "  model.add(tf.keras.layers.Dense(units=hp_units1,\n",
    "                                        input_dim=input_dim,\n",
    "                                        activation=hp_first_activation,\n",
    "                                        kernel_regularizer=tf.keras.regularizers.l2(hp_regularization_rate))  )\n",
    "  model.add(tf.keras.layers.Dropout(hp_droput_rate))\n",
    "  model.add(tf.keras.layers.Dense(units=hp_units2,\n",
    "                                        activation='elu',\n",
    "                                        kernel_regularizer=tf.keras.regularizers.l1(hp_regularization_rate))  )\n",
    "  model.add(tf.keras.layers.Dropout(hp_droput_rate))\n",
    "  model.add(tf.keras.layers.Dense(units=hp_units3,\n",
    "                                        activation='relu',\n",
    "                                        kernel_regularizer=tf.keras.regularizers.l2(hp_regularization_rate))  )\n",
    "  model.add(tf.keras.layers.Dense(units=output_layer_dim, activation='softmax'))\n",
    "\n",
    "  # Compile created model\n",
    "  model.compile(optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate),\n",
    "                loss = \"categorical_crossentropy\", \n",
    "                metrics = ['accuracy'])\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "source": [
    "### Initiate the tuner"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save logs and checkpoints in /keras_tuners/mist_playgrand\n",
    "tuner = kt.Hyperband(build_model_hp,\n",
    "                     objective = 'val_accuracy', \n",
    "                     max_epochs = 10,\n",
    "                     factor = 3,\n",
    "                     directory = 'keras_tuners',\n",
    "                     project_name = 'mist_playgrand') "
   ]
  },
  {
   "source": [
    "### Define callback to clear output after every step"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
    "  def on_train_end(*args, **kwargs):\n",
    "    IPython.display.clear_output(wait = True)"
   ]
  },
  {
   "source": [
    "### Create validation set used by Keras Tuner"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Randomize and split labeled data into a new train set and dev set.\n",
    "# Up till now TF took care of that during training\n",
    "new_X_train, X_dev_test, new_Y_train, Y_dev_test = train_test_split(\n",
    "    X_train, Y_train, test_size=0.20, random_state=17)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### Hyperparameter search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuner.search(new_X_train, new_Y_train, epochs = 10, validation_data = (X_dev_test, Y_dev_test), callbacks = [ClearTrainingOutput()])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal parameters found are:\\n\n",
    "learninig rate: {best_hps.get('learning_rate')} \\n\n",
    "regularyzation rate: {best_hps.get('regularization_rate')} \\n\n",
    "droput rate: {best_hps.get('droput_rate')}\n",
    "units in first layer: {best_hps.get('units1')} \\n\n",
    "units in second layer: {best_hps.get('units2')} \\n\n",
    "units in third layer: {best_hps.get('units3')} \\n\n",
    "first activation function: {best_hps.get('activation1')} \\n\n",
    "\"\"\")"
   ]
  },
  {
   "source": [
    "### Build model using hyperparameters found by Keeras Tuner"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train, Y_train, epochs = 50, validation_data = (X_test, Y_test))"
   ]
  },
  {
   "source": [
    "### Evaluate final model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", model.evaluate(X_test, Y_test)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}